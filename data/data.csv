titre	authors	date	texte	num_comments	url	source	summary	id
Discussion Thread | January 2025	AutoModerator	2025/01/01		3.0	https://www.reddit.com//r/Coronavirus/comments/1hqyjvo/discussion_thread_january_2025/	Reddit		1
Covid surges across US after holidays amid low booster uptake | Coronavirus	koi-lotus-water-pond	2025/01/04		10.0	https://www.reddit.com//r/Coronavirus/comments/1hsw774/covid_surges_across_us_after_holidays_amid_low/	Reddit		2
China steps up monitoring of emerging respiratory diseases	antihostile	2025/01/04		4.0	https://www.reddit.com//r/Coronavirus/comments/1hsz0h2/china_steps_up_monitoring_of_emerging_respiratory/	Reddit		3
5 things we know and still don’t know about COVID, 5 years after it appeared	bostonglobe	2025/01/02		23.0	https://www.reddit.com//r/Coronavirus/comments/1hs365n/5_things_we_know_and_still_dont_know_about_covid/	Reddit		4
WHO still waiting on COVID origins data from China 	donutloop	2024/12/31		31.0	https://www.reddit.com//r/Coronavirus/comments/1hqemc8/who_still_waiting_on_covid_origins_data_from_china/	Reddit		5
Infectivity of exhaled SARS-CoV-2 aerosols is sufficient to transmit covid-19 within minutes	ConferenceChoice7900	2024/12/29		21.0	https://www.reddit.com//r/Coronavirus/comments/1hp2hfa/infectivity_of_exhaled_sarscov2_aerosols_is/	Reddit		6
New combo tests let you check for COVID and flu at home. How well do they work?	bostonglobe	2024/12/27		19.0	https://www.reddit.com//r/Coronavirus/comments/1hnfo1s/new_combo_tests_let_you_check_for_covid_and_flu/	Reddit		7
NIH-sponsored trial of nasal COVID-19 vaccine opens	BothZookeepergame612	2024/12/26		13.0	https://www.reddit.com//r/Coronavirus/comments/1hmf1xz/nihsponsored_trial_of_nasal_covid19_vaccine_opens/	Reddit		8
Flu and COVID vaccination rates in Mass. hit new lows as infections climb into the holidays	bostonglobe	2024/12/24		20.0	https://www.reddit.com//r/Coronavirus/comments/1hlfk04/flu_and_covid_vaccination_rates_in_mass_hit_new/	Reddit		9
Controversial COVID study that promoted hydroxychloroquine treatment retracted after four-year saga	fat_cock_freddy	2024/12/21		34.0	https://www.reddit.com//r/Coronavirus/comments/1hjgf8v/controversial_covid_study_that_promoted/	Reddit		10
Hierarchical Dirichlet Process and Relative Entropy	Shui Feng	2022/10/24			http://arxiv.org/abs/2210.13142v1	ArXiv	The Hierarchical Dirichlet process is a discrete random measure serving as an important prior in Bayesian non-parametrics. It is motivated with the study of groups of clustered data. Each group is modelled through a level two Dirichlet process and all groups share the same base distribution which itself is a drawn from a level one Dirichlet process. It has two concentration parameters with one at each level. The main results of the paper are the law of large numbers and large deviations for the hierarchical Dirichlet process and its mass when both concentration parameters converge to infinity. The large deviation rate functions are identified explicitly. The rate function for the hierarchical Dirichlet process consists of two terms corresponding to the relative entropies at each level. It is less than the rate function for the Dirichlet process, which reflects the fact that the number of clusters under the hierarchical Dirichlet process has a slower growth rate than under the Dirichlet process.	11
Central limit theorems associated with the hierarchical Dirichlet  process	Shui Feng,J. E. Paguyo	2024/04/24			http://arxiv.org/abs/2404.16034v1	ArXiv	The Dirichlet process is a discrete random measure specified by a concentration parameter and a base distribution, and is used as a prior distribution in Bayesian nonparametrics. The hierarchical Dirichlet process generalizes the Dirichlet process by randomizing the base distribution through a draw from another Dirichlet process. It is motivated by the study of groups of clustered data, where the group specific Dirichlet processes are linked through an intergroup Dirichlet process. Focusing on an individual group, the hierarchical Dirichlet process is a discrete random measure whose weights have stronger dependence than the weights of the Dirichlet process. In this paper, we study the asymptotic behavior of the power sum symmetric polynomials for the vector of weights of the hierarchical Dirichlet process when the corresponding concentration parameters tend to infinity. We establish central limit theorems and obtain explicit representations for the asymptotic variances, with the latter clearly showing the impact of the hierarchical structure. These objects are closely related to the homozygosity in population genetics, the Simpson diversity index in ecology, and the Herfindahl-Hirschman index in economics.	12
Spectral analysis of communication networks using Dirichlet eigenvalues	Alexander Tsiatas,Iraj Saniee,Onuttom Narayan,Matthew Andrews	2011/02/17			http://arxiv.org/abs/1102.3722v2	ArXiv	The spectral gap of the graph Laplacian with Dirichlet boundary conditions is computed for the graphs of several communication networks at the IP-layer, which are subgraphs of the much larger global IP-layer network. We show that the Dirichlet spectral gap of these networks is substantially larger than the standard spectral gap and is likely to remain non-zero in the infinite graph limit. We first prove this result for finite regular trees, and show that the Dirichlet spectral gap in the infinite tree limit converges to the spectral gap of the infinite tree. We also perform Dirichlet spectral clustering on the IP-layer networks and show that it often yields cuts near the network core that create genuine single-component clusters. This is much better than traditional spectral clustering where several disjoint fragments near the periphery are liable to be misleadingly classified as a single cluster. Spectral clustering is often used to identify bottlenecks or congestion; since congestion in these networks is known to peak at the core, our results suggest that Dirichlet spectral clustering may be better at finding bona-fide bottlenecks.	13
Dirichlet Process Mixtures of Generalized Mallows Models	Marina Meila,Harr Chen	2012/03/15			http://arxiv.org/abs/1203.3496v1	ArXiv	We present a Dirichlet process mixture model over discrete incomplete rankings and study two Gibbs sampling inference techniques for estimating posterior clusterings. The first approach uses a slice sampling subcomponent for estimating cluster parameters. The second approach marginalizes out several cluster parameters by taking advantage of approximations to the conditional posteriors. We empirically demonstrate (1) the effectiveness of this approximation for improving convergence, (2) the benefits of the Dirichlet process model over alternative clustering techniques for ranked data, and (3) the applicability of the approach to exploring large realworld ranking datasets.	14
Clustering consistency with Dirichlet process mixtures	Filippo Ascolani,Antonio Lijoi,Giovanni Rebaudo,Giacomo Zanella	2022/05/25			http://arxiv.org/abs/2205.12924v1	ArXiv	Dirichlet process mixtures are flexible non-parametric models, particularly suited to density estimation and probabilistic clustering. In this work we study the posterior distribution induced by Dirichlet process mixtures as the sample size increases, and more specifically focus on consistency for the unknown number of clusters when the observed data are generated from a finite mixture. Crucially, we consider the situation where a prior is placed on the concentration parameter of the underlying Dirichlet process. Previous findings in the literature suggest that Dirichlet process mixtures are typically not consistent for the number of clusters if the concentration parameter is held fixed and data come from a finite mixture. Here we show that consistency for the number of clusters can be achieved if the concentration parameter is adapted in a fully Bayesian way, as commonly done in practice. Our results are derived for data coming from a class of finite mixtures, with mild assumptions on the prior for the concentration parameter and for a variety of choices of likelihood kernels for the mixture.	15
A Hierarchical Dirichlet Process Model with Multiple Levels of  Clustering for Human EEG Seizure Modeling	Drausin Wulsin,Shane Jensen,Brian Litt	2012/06/18			http://arxiv.org/abs/1206.4616v1	ArXiv	Driven by the multi-level structure of human intracranial electroencephalogram (iEEG) recordings of epileptic seizures, we introduce a new variant of a hierarchical Dirichlet Process---the multi-level clustering hierarchical Dirichlet Process (MLC-HDP)---that simultaneously clusters datasets on multiple levels. Our seizure dataset contains brain activity recorded in typically more than a hundred individual channels for each seizure of each patient. The MLC-HDP model clusters over channels-types, seizure-types, and patient-types simultaneously. We describe this model and its implementation in detail. We also present the results of a simulation study comparing the MLC-HDP to a similar model, the Nested Dirichlet Process and finally demonstrate the MLC-HDP's use in modeling seizures across multiple patients. We find the MLC-HDP's clustering to be comparable to independent human physician clusterings. To our knowledge, the MLC-HDP model is the first in the epilepsy literature capable of clustering seizures within and between patients.	16
Spectral cluster asymptotics of the Dirichlet to Neumann operator on the  two-sphere	S. Pérez-Esteva,A. Uribe,C. Villegas-Blas	2024/12/21			http://arxiv.org/abs/2412.16652v1	ArXiv	"We study the spectrum of the Dirichlet to Neumann operator of the two-sphere associated to a Schr\""odinger operator in the unit ball. The spectrum forms clusters of size $O(1/k)$ around the sequence of natural numbers $k=1,2,\ldots$, and we compute the first three terms in the asymptotic distribution of the eigenvalues within the clusters, as $k\to\infty$ (band invariants). There are two independent aspects of the proof. The first is a study of the Berezin symbol of the Dirichlet to Neumann operator, which arises after one applies the averaging method. The second is the use of a symbolic calculus of Berezin-Toeplitz operators on the manifold of closed geodesics of the sphere."	17
The supervised hierarchical Dirichlet process	Andrew M. Dai,Amos J. Storkey	2014/12/17			http://arxiv.org/abs/1412.5236v1	ArXiv	We propose the supervised hierarchical Dirichlet process (sHDP), a nonparametric generative model for the joint distribution of a group of observations and a response variable directly associated with that whole group. We compare the sHDP with another leading method for regression on grouped data, the supervised latent Dirichlet allocation (sLDA) model. We evaluate our method on two real-world classification problems and two real-world regression problems. Bayesian nonparametric regression models based on the Dirichlet process, such as the Dirichlet process-generalised linear models (DP-GLM) have previously been explored; these models allow flexibility in modelling nonlinear relationships. However, until now, Hierarchical Dirichlet Process (HDP) mixtures have not seen significant use in supervised problems with grouped data since a straightforward application of the HDP on the grouped data results in learnt clusters that are not predictive of the responses. The sHDP solves this problem by allowing for clusters to be learnt jointly from the group structure and from the label assigned to each group.	18
Scalable Inference for Latent Dirichlet Allocation	James Petterson,Tiberio Caetano	2009/09/25			http://arxiv.org/abs/0909.4603v1	ArXiv	We investigate the problem of learning a topic model - the well-known Latent Dirichlet Allocation - in a distributed manner, using a cluster of C processors and dividing the corpus to be learned equally among them. We propose a simple approximated method that can be tuned, trading speed for accuracy according to the task at hand. Our approach is asynchronous, and therefore suitable for clusters of heterogenous machines.	19
Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process  Mixture	Trevor Campbell,Miao Liu,Brian Kulis,Jonathan P. How,Lawrence Carin	2013/05/28			http://arxiv.org/abs/1305.6659v2	ArXiv	This paper presents a novel algorithm, based upon the dependent Dirichlet process mixture model (DDPMM), for clustering batch-sequential data containing an unknown number of evolving clusters. The algorithm is derived via a low-variance asymptotic analysis of the Gibbs sampling algorithm for the DDPMM, and provides a hard clustering with convergence guarantees similar to those of the k-means algorithm. Empirical results from a synthetic test with moving Gaussian clusters and a test with real ADS-B aircraft trajectory data demonstrate that the algorithm requires orders of magnitude less computational time than contemporary probabilistic and hard clustering algorithms, while providing higher accuracy on the examined datasets.	20
